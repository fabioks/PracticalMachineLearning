{
  "name": "Practical Machine Learning",
  "tagline": "",
  "body": "Summary\r\n-------\r\n\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now\r\npossible to collect a large amount of data about personal activity\r\nrelatively inexpensively. These type of devices are part of the\r\nquantified self movement â€“ a group of enthusiasts who take measurements\r\nabout themselves regularly to improve their health, to find patterns in\r\ntheir behavior, or because they are tech geeks. One thing that people\r\nregularly do is quantify how much of a particular activity they do, but\r\nthey rarely quantify how well they do it. In this project, the goal is\r\nto use data from accelerometers on the belt, forearm, arm, and dumbell\r\nof 6 participants and predict the manner in which they did the exercise.\r\nThey were asked to perform barbell lifts correctly and incorrectly in 5\r\ndifferent ways.\r\n\r\nLoading necessary libraries\r\n---------------------------\r\n\r\n    library(caret)\r\n    library(rpart)\r\n    library(rpart.plot)\r\n    library(RColorBrewer)\r\n    library(rattle)\r\n    library(randomForest)\r\n    library(knitr)\r\n    library(gbm)\r\n    library(survival)\r\n    library(splines)\r\n    library(parallel)\r\n    library(plyr)\r\n\r\nLoading Data\r\n------------\r\n\r\nLoading the working data and the 20 test case.\r\n\r\n    if (!file.exists(\"pml-training.csv\")) {\r\n      download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \r\n                    destfile = \"pml-training.csv\")\r\n    }\r\n    if (!file.exists(\"pml-testing.csv\")) {\r\n      download.file(\"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \r\n                    destfile = \"pml-testing.csv\")\r\n    }\r\n\r\n    training <- read.csv(\"pml-training.csv\", sep = \",\", na.strings = c(\"NA\",\"#DIV/0!\",\"\"))\r\n    testing <- read.csv(\"pml-testing.csv\", sep = \",\", na.strings = c(\"NA\",\"#DIV/0!\",\"\"))\r\n\r\nCleaning and Preparing Data\r\n---------------------------\r\n\r\nSplitting the training and test data\r\n\r\n    inTrain <- createDataPartition(training$classe, p = 0.6, list = FALSE)\r\n    mytraining <- training[inTrain, ]\r\n    mytesting <- training[-inTrain, ]\r\n\r\nRemoving variables with near zero variance\r\n\r\n    nzv <- nearZeroVar(mytraining, saveMetrics = TRUE)\r\n    mytraining <- mytraining[, nzv$nzv == FALSE]\r\n\r\n    nzv <- nearZeroVar(mytesting, saveMetrics = TRUE)\r\n    mytesting <- mytesting[, nzv$nzv == FALSE]\r\n\r\n    mytraining <- mytraining[c(-1)]\r\n    testing <- testing[-1,]\r\n\r\nRemoving variables that are almost NA's.\r\n\r\n    trainingtemp <- mytraining\r\n    for(i in 1:length(mytraining)) {\r\n        if( sum( is.na( mytraining[, i] ) ) /nrow(mytraining) >= .7) {\r\n            for(j in 1:length(trainingtemp)) {\r\n                if( length( grep(names(mytraining[i]), names(trainingtemp)[j]) ) == 1)  {\r\n                    trainingtemp <- trainingtemp[ , -j]\r\n                }   \r\n            } \r\n        }\r\n    }\r\n\r\n    mytraining <- trainingtemp\r\n    rm(trainingtemp)\r\n\r\n    header1 <- colnames(mytraining)\r\n    header2 <- colnames(mytraining[, -58])\r\n    mytesting <- mytesting[header1]\r\n    testing <- testing[header2]   \r\n\r\nCoercing testing data into the same type.\r\n\r\n    for (i in 1:length(testing) ) {\r\n        for(j in 1:length(mytraining)) {\r\n            if( length( grep(names(mytraining[i]), names(testing)[j]) ) == 1)  {\r\n                class(testing[j]) <- class(mytraining[i])\r\n            }      \r\n        }      \r\n    }\r\n\r\n    # To get the same class between testing and myTraining\r\n    testing <- rbind(mytraining[2, -58] , testing)\r\n    testing <- testing[-1,]\r\n\r\nPrediction with Decision Trees\r\n------------------------------\r\n\r\n    set.seed(74810)\r\n    modFitA1 <- rpart(classe ~ ., data = mytraining, method = \"class\")\r\n    fancyRpartPlot(modFitA1)\r\n\r\n![](https://raw.githubusercontent.com/fabioks/PracticalMachineLearning/master/CourseProject_files/figure-markdown_strict/unnamed-chunk-7-1.png)\r\n\r\n    predictionsA1 <- predict(modFitA1, mytesting, type = \"class\")\r\n    cmtree <- confusionMatrix(predictionsA1, mytesting$classe)\r\n    cmtree\r\n\r\n    ## Confusion Matrix and Statistics\r\n    ## \r\n    ##           Reference\r\n    ## Prediction    A    B    C    D    E\r\n    ##          A 2152   76    8    3    0\r\n    ##          B   58 1248   79   59    0\r\n    ##          C   22  178 1250  209   66\r\n    ##          D    0   16   15  816   87\r\n    ##          E    0    0   16  199 1289\r\n    ## \r\n    ## Overall Statistics\r\n    ##                                           \r\n    ##                Accuracy : 0.8609          \r\n    ##                  95% CI : (0.8531, 0.8685)\r\n    ##     No Information Rate : 0.2845          \r\n    ##     P-Value [Acc > NIR] : < 2.2e-16       \r\n    ##                                           \r\n    ##                   Kappa : 0.824           \r\n    ##  Mcnemar's Test P-Value : NA              \r\n    ## \r\n    ## Statistics by Class:\r\n    ## \r\n    ##                      Class: A Class: B Class: C Class: D Class: E\r\n    ## Sensitivity            0.9642   0.8221   0.9137   0.6345   0.8939\r\n    ## Specificity            0.9845   0.9690   0.9267   0.9820   0.9664\r\n    ## Pos Pred Value         0.9611   0.8643   0.7246   0.8737   0.8570\r\n    ## Neg Pred Value         0.9857   0.9578   0.9807   0.9320   0.9759\r\n    ## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n    ## Detection Rate         0.2743   0.1591   0.1593   0.1040   0.1643\r\n    ## Detection Prevalence   0.2854   0.1840   0.2199   0.1190   0.1917\r\n    ## Balanced Accuracy      0.9743   0.8956   0.9202   0.8083   0.9302\r\n\r\nPrediction with Random Forests\r\n------------------------------\r\n\r\n    set.seed(74810)\r\n    modFitB1 <- randomForest(classe ~ ., data = mytraining)\r\n\r\n    predictionB1 <- predict(modFitB1, mytesting, type = \"class\")\r\n    cmrf <- confusionMatrix(predictionB1, mytesting$classe)\r\n    cmrf\r\n\r\n    ## Confusion Matrix and Statistics\r\n    ## \r\n    ##           Reference\r\n    ## Prediction    A    B    C    D    E\r\n    ##          A 2232    1    0    0    0\r\n    ##          B    0 1517    2    0    0\r\n    ##          C    0    0 1365    3    0\r\n    ##          D    0    0    1 1282    1\r\n    ##          E    0    0    0    1 1441\r\n    ## \r\n    ## Overall Statistics\r\n    ##                                           \r\n    ##                Accuracy : 0.9989          \r\n    ##                  95% CI : (0.9978, 0.9995)\r\n    ##     No Information Rate : 0.2845          \r\n    ##     P-Value [Acc > NIR] : < 2.2e-16       \r\n    ##                                           \r\n    ##                   Kappa : 0.9985          \r\n    ##  Mcnemar's Test P-Value : NA              \r\n    ## \r\n    ## Statistics by Class:\r\n    ## \r\n    ##                      Class: A Class: B Class: C Class: D Class: E\r\n    ## Sensitivity            1.0000   0.9993   0.9978   0.9969   0.9993\r\n    ## Specificity            0.9998   0.9997   0.9995   0.9997   0.9998\r\n    ## Pos Pred Value         0.9996   0.9987   0.9978   0.9984   0.9993\r\n    ## Neg Pred Value         1.0000   0.9998   0.9995   0.9994   0.9998\r\n    ## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n    ## Detection Rate         0.2845   0.1933   0.1740   0.1634   0.1837\r\n    ## Detection Prevalence   0.2846   0.1936   0.1744   0.1637   0.1838\r\n    ## Balanced Accuracy      0.9999   0.9995   0.9987   0.9983   0.9996\r\n\r\n    plot(modFitB1)\r\n\r\n![](https://raw.githubusercontent.com/fabioks/PracticalMachineLearning/master/CourseProject_files/figure-markdown_strict/unnamed-chunk-8-1.png)\r\n\r\nPrediction with Generalized Boosted Regression\r\n----------------------------------------------\r\n\r\n    set.seed(74810)\r\n    fitControl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 1)\r\n\r\n    gbmFit1 <- train(classe ~ ., data = mytraining, method = \"gbm\", trControl = fitControl, verbose = FALSE)\r\n    gbmFinMod1 <- gbmFit1$finalModel\r\n\r\n    gbmPredTest <- predict(gbmFit1, newdata = mytesting)\r\n    gbmAccuracyTest <- confusionMatrix(gbmPredTest, mytesting$classe)\r\n    gbmAccuracyTest\r\n\r\n    ## Confusion Matrix and Statistics\r\n    ## \r\n    ##           Reference\r\n    ## Prediction    A    B    C    D    E\r\n    ##          A 2232    7    0    0    0\r\n    ##          B    0 1500    1    0    0\r\n    ##          C    0    4 1357    2    0\r\n    ##          D    0    7   10 1281    5\r\n    ##          E    0    0    0    3 1437\r\n    ## \r\n    ## Overall Statistics\r\n    ##                                           \r\n    ##                Accuracy : 0.995           \r\n    ##                  95% CI : (0.9932, 0.9965)\r\n    ##     No Information Rate : 0.2845          \r\n    ##     P-Value [Acc > NIR] : < 2.2e-16       \r\n    ##                                           \r\n    ##                   Kappa : 0.9937          \r\n    ##  Mcnemar's Test P-Value : NA              \r\n    ## \r\n    ## Statistics by Class:\r\n    ## \r\n    ##                      Class: A Class: B Class: C Class: D Class: E\r\n    ## Sensitivity            1.0000   0.9881   0.9920   0.9961   0.9965\r\n    ## Specificity            0.9988   0.9998   0.9991   0.9966   0.9995\r\n    ## Pos Pred Value         0.9969   0.9993   0.9956   0.9831   0.9979\r\n    ## Neg Pred Value         1.0000   0.9972   0.9983   0.9992   0.9992\r\n    ## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n    ## Detection Rate         0.2845   0.1912   0.1730   0.1633   0.1832\r\n    ## Detection Prevalence   0.2854   0.1913   0.1737   0.1661   0.1835\r\n    ## Balanced Accuracy      0.9994   0.9940   0.9955   0.9964   0.9980\r\n\r\nPredicting Results on the Test Data\r\n-----------------------------------\r\n\r\nRamdom Forests gave an Accuracy of 99.71% so this model was used to\r\npredict the 20 cases.\r\n\r\n    predictionB2 <- predict(modFitB1, testing, type = \"class\")\r\n    predictionB2\r\n\r\n    ## 21  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n    ##  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n    ## Levels: A B C D E\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}